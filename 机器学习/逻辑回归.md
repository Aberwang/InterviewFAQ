## 逻辑回归模型
逻辑回归是一种分类模型，根据输入$x\in R^n$，来判断输出Y的类别（Y=1，正类；Y=0，负类），参数为权值$w\in R^n$和偏置$b\in R$。

逻辑回归把输出作为随机变量Y对于输入X的条件概率，取值范围限定在[0,1]，一般情况下，若大于0.5则Y=1，否则Y=0。从广义的线性模型角度考虑，$w\cdot x +b$ 是线性回归的输出，但它的取值范围是全体实数，
逻辑回归为了把输出的范围限定在[0,1]，引入logistic函数：
$$ g(x) = \frac {1}{1+e^{-x}} $$
logistic函数单调且可微，x趋向正无穷时，g(x)趋向1；x趋向负无穷时，g(x)趋向0；x=0时，g(x)=0.5。



Y对于X的条件概率分布如下：

$$P(Y=1|X) = \frac {1}{1+e^{-(w\cdot x +b)}}$$
$$P(Y=0|X) = \frac {e^{-(w\cdot x +b)}}{1+e^{-(w\cdot x +b)}}$$

logistic函数又称为对数几率函数。几率（odds）定义为：事件发生概率和不发生概率之间的比值，即$odds = \frac p {1-p}$。据此，可以计算得到：
$$ln\frac {P(Y=1|X)}{1 - P(Y=0|X)} = w\cdot x +b$$

换句话说，逻辑回归就是将线性回归的输出$w\cdot x +b$来拟合对数几率函数（logistic函数），核心就是将线性回归输出的取值范围从全体实数映射到概率取值[0,1]之间。

## 最大似然估计参数
逻辑回归模型学习时，给定数据集$(x_1,y_1),(x_2,y_2), ... ,(x_n,y_n)$，用最大似然法来估计参数。

单个样本的概率 $p_i = P(Y=1|X)^{y_i}P(Y=0|X)^{1-y_i}$，所以似然函数为：
$$\prod_{i=1}^n P(Y=1|X)^{y_i}P(Y=0|X)^{1-y_i}$$
设logistic函数为g(x), 对数似然函数为：
$$\sum_{i=1}^n [y_i log(g(x)) + (1-y_i)log(1-g(x))]$$ 

要最大化对数似然函数，即最小化：
$$- \frac 1n \sum_{i=1}^n [y_i log(g(x)) + (1-y_i)log(1-g(x))]$$

上式即交叉熵（Cross-Entropy）损失函数，通过梯度下降等方法可以解这个最优化问题，得到逻辑回归的参数w,b。

## 多分类
多分类时，可以构建k-1个逻辑回归模型；如果每个类互斥，logistic回归扩展成softmax回归。设有k个分类，第i个类的概率为：
$$P(y=i|x, \theta) = \frac{e^{\theta_i^T x}}{\sum_j^K{e^{\theta_j^T x}}}$$
把logistic函数替换成softmax函数，将概率归一化，损失函数为交叉熵：
$$J(\theta) = -\frac{1}{N} \sum_i^N \sum_j^K {y_i \log{p_i}}$$
可用梯度下降等方法求解。

## 冗余特征
在模型训练时，一般会进行特征选择，去除冗余特征和高度相关特征。一是能降低维度，减小计算量。二是冗余特征没有增加数据的信息，但其效果也会叠加在模型中，使效果变差。

对于逻辑回归，考虑对数几率$$log\frac{p}{1-p} = w\cdot x$$
如果所有特征重复一遍，则$w\cdot x$的值变为原来的两倍，即对数几率变为原来的两倍，显然会影响最终的分类结果。

## 优缺点
* 速度快
* 可解释性强，每个特征对应一个权重
* 无法处理正负样本不平衡的问题（全部预测为正，损失函数依然很小）
* 准确率相对不高
* 不引入其他方法时，只能处理二分类问题
