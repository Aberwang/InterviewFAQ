# 机器学习基本知识总结

## 1. 机器学习基本理论

### 1.1 vc维

标签：机器学习的本质，霍夫定不等式，成长函数，break-point，vc-维

vc维的引入，是为了证明机器学习是能够学到东西的。我们认为，机器学习的目标是从现有的数据当中学习出符合现有数据的规律g，而这个g与实际的数据规律f之间肯定是有差距的。

这样我们可以引申出两个概念：Ein，Eout，这两个概念是用来衡量一个假设函数的，Ein指的是假设函数在训练数据上的误差，Eout指的是假设函数在整个数据分布上的误差。我们学到一个假设函数以后，有两个目标：一是让Ein尽量小，而是要让Ein与Eout尽量一致，不能相差太大。

那么我们如何保证这种差距是比较小的呢？这里我们可以假设有一个罐子，每个罐子里面有黄色，绿色的珠子，黄色的珠子表示g得到的结果与f不一致，绿色的表示一致，那么我们可以认为每个罐子代表一个假设g。因为罐子中的珠子太多，我们不能全部拿出来，因此我们只能抽样查看绿色珠子的比例。那么又有一个问题，就是如何衡量抽样带来的误差，即我抽样出来的黄，绿珠子的比例与这个假设罐子中的真实比例差距有多大？关于这个，有霍夫定不等式，能够保证，抽样比例和实际比例之间的误差是有一个上界的。这就保证我对一个罐子进行抽样的时候，得到关于这个假设罐子的Ein与Eout的差距比较大的可能性是比较小的。但是，我们在假设空间寻找假设函数的时候，是对无穷个罐子进行搜索，这样，我们保证没有一个罐子会抽样到Ein与Eout相差比较大的情况的概率，就会被放大。这个放大的倍数就是罐子的个数，也就是假设空间中假设的个数。

但是其实这又是一个比较宽松的上界，在这个上界的系数是描述假设空间中假设的个数，但是其实我们想象一下线性分类（以下距离都是以线性分类为例），将分割平面稍微转动一下，就会是另外的假设，这这些假设是非常相似的，是有很大交集的，那么为了让这个上限更紧确些。我们需要重新考虑如何衡量假设空间的复杂度。这时候我们对假设空间中的函数做一下分类，即将数据点分割成同样结果的作为一类，用假设空间中函数的类别来衡量其复杂度。

按照数据点的多少，假设的类别肯定是变化的。而且数据点在空间排列的不同，数据点所属的类别不同，假设的类别肯定也是不同的。我们考虑在给定数据个数的情况下，在不同的数据分布的情况下，假设空间最多能有多少类。这是一个关于数据个数的函数。例如在数据点为3的时候, 下图显示了两种不同的数据分布，左图的分布情况下，八种情况都可以线性可分，这时假设空间中假设有8类，右图中，第三行的情况是线性不可分的，因此在当前的数据分布情况下，假设空间中只有6类假设。因此，在数据个数为3的情况下，假设空间的类别数为8。在这里，我们将这种描述假设空间类别的函数称为 成长函数（按照数据点的个数进行成长的）。

![vc_1](/image/vc_1.png)

按照成长函数的定义，我们自然想到成长函数的上限是2^n,但是指数增长是非常快的，我们是否能够找到一个更加紧确的上限呢？这里我们提出了叫做“shater”的概念，指的是在数据量一定的情况下，存在一种数据分布情况，分割平面能够将数据空间中的所有点标记的所有情况都分割成功（即上图这种情况）。随着数据量的增加，肯定存在一个数据量的情况（我们称这时候的数据点的个数为break-point），假设空间不能够shater所有的数据点，这时候，我们的假设空间的类别个数就不再是指数增加了，说不定就变成多项式了呢。。（先小小的幻想一下，后面证明的确是多项式）

上段中提到了break-point的概念，这里举几个例子，具体说明一下什么是break-point。例如在假设空间是正向射线的情况下，在数据点为2的情况下，假设空间有三种类别：起始点分别位于比两个数据点都小，两个数据点中间，比两个数据点都大。这时候假设空间的类别为3，不等于2^2，因此break-point为2。

![vc_2](/image/vc_2.png)

知道了成长函数不总是指数增长的，接下来我们想要证明出来一个比较紧确的多项式的上界。我们可以将成长函数的上限表示成B(N,k),其中N是数据点的个数，k是break-point。B(N,k)的概念就是在一定数据量的情况下，将数据点的标记情况进行排列组合，排列组合的结果不能shater到k个点（也就是不能将k个点中的所有情况都拍哦咧组合出来）。由此我们可以想象到，B(N,k)是一个动态规划的情况，有以下递推公式：

![vc_3](/image/vc_3.png)

现在，我们证明了成长函数的上界的上界是一个多项式的表达式，因此，我们接下里的问题就是是否可以利用成长函数的上界来将Ein和Eout的差别很大的概率限制到一个更加紧确的上界。这里我就不加证明了，肯定是可以的。详细可以看林老师的机器学习基石的第六节。

接下来，我们看面试中常问到的vc维。vc维的定义就是假设空间的类别个数等于2^n的最大数据量，就是break-point减1.

### 1.2 概率论基础

概率分布在机器学习中占有非常重要的作用。尤其是在贝叶斯相关的理论中，参数，训练数据，预测变量等都是在一定的概率分布下形成的，我们预测的目标也主要是在对概率分布建模。而现实生活中的概率分布是非常复杂的，而我们只能利用简单的概率分布对其进行估计，近似。最常用的，最基本的一些概率分布模型就是指数族的概率分布了，包括很多基本的概率分布。

#### 1.2.1 常见的概率分布

标签：beta分布，二项分布，泊松分布，正态分布，狄利克雷分布，极限关系，共轭关系，一致性

伯努利分布：一次抛硬币实验中（硬币未必均匀），投掷结果的概率分布；

二项分布：伯努利试验重复多次，投掷结果就服从二项分布；多次抛硬币实验中，投掷结果的概率分布。

泊松分布：泊松分布描述的是单位时间内某件小概率事件发生次数的分布。判断一件事情是否服从于泊松分布的条件：某件事情发生属于小概率事件，某件事情的发生是独立的，某件事情发生的概率是稳定的。泊松分布的表达可以由二项分布推导出来，当二项分布中n趋向于无穷，p趋向于0，而np是常数。因此，泊松分布可以理解为单位时间内无穷多次的伯努利实验，这些伯努利实验的和是稳定的，为lambda。

指数分布：主要描述相互独立的随机事件之间的时间间隔的概率；指数分布的形式可以由泊松分布推导出来。

gamma分布：gamma分布是独立同分布的指数分布的积分。gamma分布可以理解从头开始，到第n次事件发生的时间的分布。

beta分布：一定范围内的n个随机变量进行排序后，第k个变量的取值就是一个beta分布，其中alpha = k , beta = n - k + 1。其中gamma函数起到了将变量从整数空间延拓到实数空间的作用。

狄利克雷分布：一定范围内的n个随机变量进行排序后，第k1个变量，第k2个变量的联合分布就是一个狄利克雷分布。

高斯分布：在n趋近于无穷的时候，随机变量的均值收敛于正态分布。

卡方分布：n个服从正态分布的随机变量的平方和的分布。

t分布：x服从标准正态分布，y服从卡方分布。

F分布：x服从自由度为n的卡方分布，y服从自由夫为m的卡方分布。

#### 1.2.2 常用的统计学检验方法

t检验：适用于方差齐性的两组小样本之间的比较。

f检验：

卡方检验：理论频数和实际频数的吻合程度

p值：

自由度：

### 1.3模型选择

标签：F1，AUC，ROC，k折交叉验证，AIC，BIC，L1，L2，bootstrapping

#### 1.3.1 性能度量

回归任务中常用的性能度量是最小平方差

分类任务中，由于我们一般通过混淆矩阵来定义不同的度量方式：

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |
|      |      |      |

#### 1.3.2 模型选择方法

#### 1.3.3 过拟合的处理方法

## 2. 机器学习常见算法：

标签：原理，贝叶斯，优缺点，使用场景

### 2.1 广义线性回归。 

标签：最小二乘，最大似然，高斯噪声先验，基函数，后验最大化

线性回归是最简单的机器学习模型了，我们将目标值表示成特征向量的加权组合，重要的特征加权大，不重要的特征权重小。然后，我们根据目标值域预测值之间的差距，以其平方差来衡量错误的大小，然后我们将这个错误最小化（凸函数梯度为0），可以求出w的闭式解。这就是最简单的线性回归了。

但是我们肯定知道，这样简单的模型是存在问题的。第一，我们学出来的上面的模型永远是特征的线性模型，为了解决更复杂的问题，我们需要将非线性带入模型中。第二，我们为什么要用平方差来衡量误差错误？仅仅是因为这样我们可以得到一个能够有闭式解的凸优化问题吗？第三，模型的过拟合应该如何处理？

首先，我们回答第一个问题。为了解决更复杂的问题，我们可以对输入空间（特征空间）做一个变换，加上非线性的变换，例如高斯变换，sigmoid变换等等，但是整个模型仍然是w的线性模型，也就是说我们的广义线性模型是针对于w而言的。

第二个问题，我们从概率论的角度说起。我们在回归任务中，是对p(t|x)进行建模，这样根据我们的广义线性模型，我们认为目标值t服从以线性回归结果为均值，beta为精度的高斯分布（beta为噪声）。然后，我们的预测值为这个高斯分布的期望，也就是广义线性回归的结果。在优化的时候，在概率论的支撑下，我们可以以最大似然估计为准则来进行模型的学习。那么结论就要来了，在高斯噪声假设下最大似然估计的w的结果就是最小化平方差的结果，而beta的估计结果就是回归函数周围的残差方差。平方差误差函数还可以理解为预测目标与线性预测结果在欧氏空间的距离，因此最小平方差的解就是使得预测目标和线性预测目标之间的欧氏距离最短，不难证明，最近的预测值就是预测目标在特征空间的投影。

第三个问题，就是要引入在机器学习模型中占有重大比重的正则项。在现在的模型中，基本已经没有不考虑正则项的。正则项的存在意义可以从两个方向进行理解：一个是控制模型的复杂度，降低模型的vc-维；另外一个从贝叶斯角度出发，可以理解带有正则项的目标函数等于以高斯分布为先验的w的后验分布的最大似然表示(这里我们下一段再展开讲)。正则项一般可以考虑L1，L2正则，当然也有L0正则，只过限于不好优化不经常被用。L1，L2正则当然都能够起到降低模型复杂度的作用。L1正则是从稀疏模型的角度出发，由于其在损失函数等高线上，可以表示成方块的限制条件，这样在边界取极值时，一般会在坐标轴上取到，即会导致某个维度权重为0，从而有稀疏化，特征选择的作用。L2正则化是权重衰减，不想让权重很大。L2正则化在线性模型中能够让w总有闭式解，同时也因为L2是凸函数，这样的模型比较好优化，因此应用比较广泛。

接下来我们可以从贝叶斯的角度去理解回归模型。

最大后验概率模型：给w一个先验概率分布：高斯分布，均值为0，精度为alpha，预测目标t也是高斯分布，这样我们可以得到w的后验概率分布也是高斯分布。同时我们可以得到w后验概率分布的形式。这时w后验概率的最小化相当于似然函数加上正则项（参数为alpha／beta）的最小化。

那么我们在进行模型的选择的时候，一般是要对正则化系数进行交叉验证然后求解的。然而，如果我们利用贝叶斯的观点，从最大化边缘似然函数的角度出发，则完全可以由训练数据确定超参数alpha和beta。

广义线性模型的局限性：广义线性模型的局限性主要在于基函数在被观测之前就已经确定了下来，

### 2.2 logistic regression

标签：sigmoid函数，logit，beta先验，狄利克雷先验，最大熵原则，简单，精确度低，离散化特征

原理：假设目标变量服从二项分布，错误用交叉熵来衡量，不用

贝叶斯角度的原理：

为什么用的sigmoid函数与最大熵模型的关系：

优缺点：

使用场景：

为什么使用离散化特征：

0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。

### 2.3 svm

标签：最大间隔，对偶，kkt条件，软间隔，SMO算法，核技术

2.4 核技术

### 2.4 决策树模型

标签：信息增益，ID3，C4.5，CART，信息增益比，gini系数，回归树，平方差，过拟合，剪枝。

### 2.5 随机森林

标签：bagging，投票，方差，深度，随机

### 2.6 gbdt-xgboost

标签：boosting，回归树，残差，函数空间的梯度下降，一阶，二阶泰勒展开，

### 2.7 优化算法 

标签：随机梯度下降，共轭梯度下降，Adam，梯度截断，ftrl

这里讲的优化主要是凸优化，也就是优化问题里面最简单的问题，我们的核心思想就是尽可能让机器学习求解的问题变成凸优化，但是我们这篇文章主要讲的是在机器学习中凸优化的求解方式。

机器学习中的凸优化一般都是比较简单的，不过比较复杂的就是带有约束条件的凸优化。对于简单的凸优化，我们可以用梯度下降直接求解，(梯度下降也是有很多扩展的,下一段我们会讲到)；对于带有限制条件的凸优化，我们一般是采取拉格朗日乘子，KKT条件等，将问题转化为不带约束条件的凸优化进行求解。

带有约束条件的凸优化，比较经典的推导是在SVM的原理推导中出现的，而且由

2.7.2 梯度下降

随机梯度下降

共轭梯度下降

梯度下降的改进：Adama，Ada

2.7.3 在线学习算法

online最优化求解，很难产生稀疏解，精确度，稀疏性

TG

截断梯度

L1-FOBOS

L1-RAD

FRTL的考虑，将

参数的设置

### 2.8 神经网络

标签：激活函数，非线性，后向传播算法

### 2.9 HMM算法

### 2.10 条件随机场

### 2.11 聚类：狄利克雷过程。。聚类

### 2.12 降维

### 2.13 频繁模式挖掘

## 3. 贝叶斯推断学习中的推断方法

3.1 参数化贝叶斯

最大后验分布

最大边缘概率分布

3.2 非参数贝叶斯

两者的不同在于先验的不同

## 3.项目相关重点算法

标签：如何应用，场景是什么。

3.1 推荐系统相关算法

3.1.1 FFM wide&deep FFM + wide & deep

3.1.2 ftrl原理和应用：

由于L1正则在online模式下不能够产生较好的稀疏性

3.2 自然语言相关算法

分词算法

主题模型相关算法

## 4.机器学习实践技巧

4.1 数据异常点检测

4.2 数据过拟合处理方式

4.3 特征工程，特征选择技巧





